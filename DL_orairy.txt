 ・(単純)パーセプトロン：隠れ層を持たず、活性化関数はステップ関数
 ・ニューラルネットワーク：隠れ層を持ち、活性化関数はシグモイド関数やReLU関数(Rectified Linear Unit) 多層パーセプトロンともいう

・丸め誤差(rounding error)
 少数の小さな範囲において、数値が省略されることで(例えば小数点第8位以下が省略)、最終的な計算結果に誤差が生じること 10^(-4)なら微小値として問題ない
 np.float32(1e-50) → 0.0

 ・勾配(gradient)
  すべての変数の偏微分をベクトルとしてまとめたものを勾配と呼び、勾配が示す方向は各場所において、関数の値を最も減らす方向である。

・勾配法
 繰り返し勾配方向に進み、関数の値を徐々に減らす方法
    ・勾配降下法：最小値を探す場合
    ・勾配上昇法：最大値を探す場合 ※損失関数の符号を反転させれば、同じなので勾配降下法が一般的

・ハイパーパラメータ：人の手で学習前に決定しておくパラメータ(評価指標,エポック数,学習率,閾値,身にバッチサイズ,層の数など)
・パラメータ：機械学習モデルが学習過程で自動的に調整するもの(重み,バイアスなど)


・連鎖率の原理：合成関数の微分は、それぞれの関数の微分の積(偏微分)によって表せる
                σz/σx = σz/σt ・ σt/σx

・逆伝播法の計算：ノードの入力信号に対して、ノードの局所的な微分(偏微分)を乗算して次のノードに伝達する
                連鎖率の原理を使って目的とする微分の値を効率よく求めることができる
    
    ・加算ノードの逆伝播：z=x+y のとき
                         σz/σx = 1
                         σz/σy = 1 となり、入力された値に1を掛けて、そのまま次のノードに流す
    
    ・乗算ノードの逆伝播： z=xy のとき
                         σz/σx = y
                         σz/σy = x となり、入力された値に順伝播の入力信号をひっくり返した値を掛けて、次のノードに流す
                         10         6.5
                            ↘           ↖
                              50          1.3
                            ↗           ↙
                         5           13

・Affine(全結合層)：入力されたすべての行列の内積(ドット積)と重みの和



6章 パラメータの更新
・SGDの欠点：関数の形状が楕円などの伸びた形だと、非効率な経路で勾配方向へ進み、勾配の方向が最小値ではない方向を指してしまう ジグザグに下る
            Stochastic Gradient Descent(確率的勾配降下法)

・Momentum(運動量)：物体が勾配方向に力を受け、その力によって物体の速度vが重みWに加算される ボールが地面の傾斜を転がるイメージ ジグザグの動きが抑えられ、SGDよりも速く下れる

・AdaGrad：学習係数の減衰(学習係数は小さいと学習に時間がかかり、大きいと発散して正しい学習が行えない)で、最初は大きく学習し、次第に小さく学習する 
            パラメータの要素ごとに適応的(Adaptive)に学習係数を調整しながら学習を行う
            過去の勾配の二乗和としてすべて記録する
            y軸方向は勾配が大きいため、最初は大きく動くが、それに比例してy軸方向への更新度合いは弱められていき、ジグザグが軽減される

    ・RMSProp：AdaGradは学習を進めれば進めるほど、更新度合いは小さくなり、無限に学習を行うと更新料は0になり、全く更新されなくなる
              この問題を解決したのがRMSPropで、過去の全ての勾配を均一に加算していくのではなく、過去の勾配を指数関数的に忘れて、新しい勾配の情報が大きく反映されるように加算する

・Adam( AdaGrad + ・Momentum )：2015年に提案された新しい手法で、移動平均で振動を抑制するMomentum と 学習率を調整するAdaGradを組み合わせたもの


・勾配消失：sigmoid関数の出力が0か1に近づくにつれて、逆伝播での勾配の値が減っていくこと
・表現力の制限：複数のニューロンがほぼ同じ値を出力すると、複数のニューロンが存在する意味がなくなり、勾配消失しなくても学習が上手くいかない 適度な広がりが必要
・Xavierの初期値：Xavierの論文では各層の出力を広がりのある分布にするために、前層のノード数をnとして、1/√n の標準偏差を持つ分布を使う(sigmoid関数, tanh関数)
                  層が増えるといびつな形になるが、sigmoid関数ではなくtanh関数を使うと均一な分布になる

・Heの初期値：Kaiming Heらが提唱した √2/n の標準偏差とするガウス分布を使う ReLU関数は値域が負の領域まで増えるからより広がりを持たせるために、標準偏差が2倍


・Batch Normalization：2015年に提唱され、各層で適度な広がりを持つように"強制的に"分布を調整する手法 ミニバッチごとにデータの分布が平均値が0, 分散が1になるように正規化をする
                        ・学習の高速化(学習係数を大きくすることができる)
                        ・初期値にそれほど依存しない
                        ・過学習を抑制する(Drop out等の必要性を減らせる)


・過学習が起こる原因：・パラメータを大量に持ち、表現力の高い複雑なモデルであること(層やニューロンの個数が多いモデル)
                     ・訓練データが少ないこと

・Weight decay(荷重減衰)：重みの値を小さくすることで、過学習が起きにくくなる
                          なお、重みの初期値を全て0 (全て均一な値)にすると、誤差逆伝播法ですべての重みの値が均一に更新されてしまう
    ・過学習を防ぐための正則化項(ノルム：いろいろなものの大きさを表す量) 損失関数に足して、値を減らすのが目的
      ・L2ノルム：√各要素の二乗和 よく使われる 荷重減衰はλW^2/2 (重みW, 正則化の強さλ) 1/2は微分後にλW^2にするため
      ・L1ノルム：各要素の絶対値の和
      ・L∞ノルム：各要素の絶対値の中で最大の値

・Drop out：学習時にニューロンをランダムに消去しながら学習する手法
            テスト時はDrop outしない代わりに、訓練時に消去した割合を掛けて出力する
            学習時に異なるモデルを学習し、テスト時にモデルの平均をとることで、1つのネットワークで疑似的にアンサンブル学習を行っているとみなせる


・訓練データ：パラメータ(重みやバイアス)の学習に使う
・検証データ：ハイパーパラメータ(ニューロン数やバッチサイズ、学習係数)の性能を評価するために使う
・テストデータ：汎化性能を確認するために、最後に使う(理想は1度だけ)



7章 畳み込みニューラルネットワーク(CNN)
・Affine-ReLUだったのが、CNNではConvolution-ReLU-(Pooling)