 ・(単純)パーセプトロン：隠れ層を持たず、活性化関数はステップ関数
 ・ニューラルネットワーク：隠れ層を持ち、活性化関数はシグモイド関数やReLU関数(Rectified Linear Unit) 多層パーセプトロンともいう

・丸め誤差(rounding error)
 少数の小さな範囲において、数値が省略されることで(例えば小数点第8位以下が省略)、最終的な計算結果に誤差が生じること 10^(-4)なら微小値として問題ない
 np.float32(1e-50) → 0.0

 ・勾配(gradient)
  すべての変数の偏微分をベクトルとしてまとめたものを勾配と呼び、勾配が示す方向は各場所において、関数の値を最も減らす方向である。

・勾配法
 繰り返し勾配方向に進み、関数の値を徐々に減らす方法
    ・勾配降下法：最小値を探す場合
    ・勾配上昇法：最大値を探す場合 ※損失関数の符号を反転させれば、同じなので勾配降下法が一般的

・ハイパーパラメータ：人の手で学習前に決定しておくパラメータ(評価指標,エポック数,学習率,閾値,ミニバッチサイズ,層の数など)
・パラメータ：機械学習モデルが学習過程で自動的に調整するもの(重み,バイアスなど)


・連鎖率の原理：合成関数の微分は、それぞれの関数の微分の積(偏微分)によって表せる
                σz/σx = σz/σt ・ σt/σx

・逆伝播法の計算：ノードの入力信号に対して、ノードの局所的な微分(偏微分)を乗算して次のノードに伝達する
                連鎖率の原理を使って目的とする微分の値を効率よく求めることができる
    
    ・加算ノードの逆伝播：z=x+y のとき
                         σz/σx = 1
                         σz/σy = 1 となり、入力された値に1を掛けて、そのまま次のノードに流す
    
    ・乗算ノードの逆伝播： z=xy のとき
                         σz/σx = y
                         σz/σy = x となり、入力された値に順伝播の入力信号をひっくり返した値を掛けて、次のノードに流す
                         10         6.5
                            ↘           ↖
                              50          1.3
                            ↗           ↙
                         5           13

・Affine(全結合層)：入力されたすべての行列の内積(ドット積)と重みの和



6章 パラメータの更新
・SGDの欠点：関数の形状が楕円などの伸びた形だと、非効率な経路で勾配方向へ進み、勾配の方向が最小値ではない方向を指してしまう ジグザグに下る
            Stochastic Gradient Descent(確率的勾配降下法)

・Momentum(運動量)：物体が勾配方向に力を受け、その力によって物体の速度vが重みWに加算される ボールが地面の傾斜を転がるイメージ ジグザグの動きが抑えられ、SGDよりも速く下れる

・AdaGrad：学習係数の減衰(学習係数は小さいと学習に時間がかかり、大きいと発散して正しい学習が行えない)で、最初は大きく学習し、次第に小さく学習する 
            パラメータの要素ごとに適応的(Adaptive)に学習係数を調整しながら学習を行う
            過去の勾配の二乗和としてすべて記録する
            y軸方向は勾配が大きいため、最初は大きく動くが、それに比例してy軸方向への更新度合いは弱められていき、ジグザグが軽減される

    ・RMSProp：AdaGradは学習を進めれば進めるほど、更新度合いは小さくなり、無限に学習を行うと更新量は0になり、全く更新されなくなる
              この問題を解決したのがRMSPropで、過去の全ての勾配を均一に加算していくのではなく、過去の勾配を指数関数的に忘れて、新しい勾配の情報が大きく反映されるように加算する

・Adam( AdaGrad + ・Momentum )：2015年に提案された新しい手法で、移動平均で振動を抑制するMomentum と 学習率を調整するAdaGradを組み合わせたもの


・勾配消失：sigmoid関数の出力が0か1に近づくにつれて、逆伝播での勾配の値が減っていくこと
・表現力の制限：複数のニューロンがほぼ同じ値を出力すると、複数のニューロンが存在する意味がなくなり、勾配消失しなくても学習が上手くいかない 適度な広がりが必要
・Xavierの初期値：Xavierの論文では各層の出力を広がりのある分布にするために、前層のノード数をnとして、1/√n の標準偏差を持つ分布を使う(sigmoid関数, tanh関数)
                  層が増えるといびつな形になるが、sigmoid関数ではなくtanh関数を使うと均一な分布になる

・Heの初期値：Kaiming Heらが提唱した √2/n の標準偏差とするガウス分布を使う ReLU関数は値域が負の領域まで増えるからより広がりを持たせるために、標準偏差が2倍


・Batch Normalization：2015年に提唱され、各層で適度な広がりを持つように"強制的に"分布を調整する手法 ミニバッチごとにデータの分布が平均値が0, 分散が1になるように正規化をする
                        ・学習の高速化(学習係数を大きくすることができる)
                        ・初期値にそれほど依存しない
                        ・過学習を抑制する(Drop out等の必要性を減らせる)


・過学習が起こる原因：・パラメータを大量に持ち、表現力の高い複雑なモデルであること(層やニューロンの個数が多いモデル)
                     ・訓練データが少ないこと

・Weight decay(荷重減衰)：重みの値を小さくすることで、過学習が起きにくくなる
                          なお、重みの初期値を全て0 (全て均一な値)にすると、誤差逆伝播法ですべての重みの値が均一に更新されてしまう
    ・過学習を防ぐための正則化項(ノルム：いろいろなものの大きさを表す量) 損失関数に足して、値を減らすのが目的
      ・L2ノルム：√各要素の二乗和 よく使われる 荷重減衰はλW^2/2 (重みW, 正則化の強さλ) 1/2は微分後にλW^2にするため
      ・L1ノルム：各要素の絶対値の和
      ・L∞ノルム：各要素の絶対値の中で最大の値

・Drop out：学習時にニューロンをランダムに消去しながら学習する手法
            テスト時はDrop outしない代わりに、訓練時に消去した割合を掛けて出力する
            学習時に異なるモデルを学習し、テスト時にモデルの平均をとることで、1つのネットワークで疑似的にアンサンブル学習を行っているとみなせる


・訓練データ：パラメータ(重みやバイアス)の学習に使う
・検証データ：ハイパーパラメータ(ニューロン数やバッチサイズ、学習係数)の性能を評価するために使う
・テストデータ：汎化性能を確認するために、最後に使う(理想は1度だけ)



7章 畳み込みニューラルネットワーク(CNN)
・Affine-ReLUだったのが、CNNではConvolution-ReLU-(Pooling) 出力層付近ではAffine-ReLU, 出力層ではAffine-Softmax
・層が深くなるにつれてニューロンは高度な情報へと変化する
  1層目：エッジ・ブロブ(色が変化する境目・塊)
  3層目：テクスチャ(材質や質感)
  5層目：物体のパーツ
  8層目：物体のクラス(イヌや車)

・全結合層の問題点：データの形状が無視されてしまうこと
                  例：画像は縦横チャンネルの3次元のデータだが、全結合層に入力する際に1次元データにする必要がある  
                      画像の3次元の形状には空間的情報が含まれる←RGBの関連性・ピクセルの位置関係など
  一方、畳み込み層は、形状を維持して三次元データをそのまま受け渡す なお、畳み込み層の入出力データを「特徴マップ」「入出力特徴マップ」という

・畳み込み層：
  ・パディング：畳み込み層の前に入力データの周囲に0などの定数を埋めること
                目的は出力サイズを調整すること
                出力サイズは入力サイズの(2,2)要素分縮小される
                例：(4,4)の入力データに(3,3)のフィルター → (2,2)の出力データ
                  もしも、畳み込み層を何度も繰り返す場合は、ある時点で出力サイズが1になってしまい、それ以上畳み込みができなくなる
                  そのため、(4,4)の入力サイズに幅1のパディングを設定すると、出力も(4,4)のまま保たれる

  ・ストライド：フィルターをずらす間隔
                例：ストライド2 (7,7)の入力データに(3,3)のフィルター → (3,3)の出力データ

    つまり、パディングを大きくすると、出力サイズは大きくなる
            ストライドを大きくすると、出力サイズは小さくなる

・プーリング層：入力データの微小な位置変化なら出力が変わらない
               ・Maxプーリング：領域内の最大値をとる
               ・Averageプーリング：領域内の平均値をとる
               一般的に、プーリングのサイズとストライドは同じ値に設定する (2×2ならストライドは2)

・データ拡張(Data Augmentation)：入力画像に回転や縦横方向の移動などを加えて、データセットを増やして拡張すること
                                ・crop処理：画像の一部を切り出す
                                ・flip処理：画像の左右を反転させる


8章 ディープラーニングの歴史
・ImageNet：100万枚を超える画像のデータセットで、画像にはラベル(クラス名)が紐づけられている
            このデータセットを使って、ILSVRC(ImageNet Large Scale Visual Recognition Challenge)という画像認識のコンペティションが2010-2017年まで行われた

・Alex Net(2012)：ディープラーニングで 25.8% → 16.4% に改善

・VGG(2014 ,2位)：畳み込み層とプーリング層からなる基本的なCNNで、16層(または19層)まで重ねているのが特徴
            3×3のフィルターによる畳み込みを2~4回連続して、プーリング層でサイズを半分にする

・Google Net(2014 ,1位)：ネットワークが横方向に広がりを持つ「インセプション構造」で、サイズの異なるフィルターとプーリングでその結果を結合する
                          また、1×1のフィルターでチャンネル方向にサイズを減らし、パラメータの削減や高速化を実現した

・Res Net(2015)：Microsoftのチームが開発したもので、スキップ構造(ショートカット・バイパスともいう)で、層を深くしすぎると学習が上手くいかない問題を解決した  
                スキップ構造とは、入力データの畳み込み層を2層ごとにスキップして出力に合算する構造
                逆伝播の際に勾配がそのまま伝わり、勾配消失問題(層が深くなると勾配が小さくなる問題)が軽減された
                実験で、150層以上深くしても認識精度は向上し続けることが分かった


活用例
・物体認識：手書き数字認識のような画像のクラス分類

・物体検出：画像の中から物体の位置と種類を分類
            ・スライディングウィンドウ方式(2012)
                物体検出の初期の手法で、画像を分割し、領域ごとに画像分類を行う
                各ウィンドウごとに画像分類を行う必要があり、計算コストが非常に大きい

            ・R-CNN(Region-based CNN features, 2014)
                1. Selective Searchを用いて物体の特徴領域を矩形表示(Region proposal:領域提案)
                2. 画像のサイズを変形し、CNNに入力(Alex Net)  ※Alex Netは227×227のサイズしか受け取らないため、強引に正方形にする
                3. 特徴量をSVM(サポートベクトル(ベクター)マシン)でクラス分類
              
            ・Faster R-CNN(2015, Microsoft)
                候補領域抽出をSelective Searchではなく、RPN(Region Proposal Network)と呼ばれるCNNにすることで、すべての処理をCNNで行い処理を高速化した
              
            ・YOLO(You Only Look Once, 2016)
                現在最も利用されている物体検出モデル
                予め画像をグリット分割し、各領域ごとに物体のクラスと位置を求めることで、Faster R-CNN以上の高速化と背景の誤差検出を減らした
                既存手法では各候補領域ごとに処理をしていたのに対し、YOLOは一度見るだけで済む
                欠点は、分割されたグリッド内で識別できるクラスが1つで、検出できる物体の数が2つまでという制約がある点

            ・SSD(Single Shot Detector)
                YOLOの次に広く使われる
                マルチスケール特徴マップという様々なスケールの特徴を利用して、1度のCNN演算で物体の領域候補検出とクラス分類を行うことで、さらに高速化
            
            ・YOLO v2(2017)
                9000もの物体を検出できるように大きな改良がされ、高精度・高速になり、YOLO9000と名付けられた
            
            ・YOLO v3(2018)
                YOLOシリーズで最も人気のあるバージョンで、v2との違いは小さな物体を検出することが得意
                処理速度や認識制度に関してはv2の方が上のこともあるが、実装が豊富でPythonから利用しやすいのが特徴


      補足
        ・IoU(Intersection of Union)：正解領域とモデルの出力した領域の重なった部分を、両者を合わせた領域で割ることで求まる指標
                                      0：全く重ならない
                                      1：ぴったり重なる

        ・mAP(mean Average Precision)：IoUが閾値より大きければTrueを返すとして、適合率(Precision)と再現率(Recall)を求めることができる
                                      Average Precision(平均適合率, AP)はPrecisionをRecallで積分すると求まる
                                      全てのクラスに対して平均を計算することで、mAPが求まる


・セグメンテーション：入力画像に対して全てのピクセルでクラス分類を行う
                    畳み込み演算で多くの領域を再計算するという無駄な計算が発生するため、FCN(Fully Convolutional Network)を使う
                    FCNは全結合層を畳み込み層に置き換え、空間ボリュームを保ったまま最後の出力まで処理する
                    逆畳み込み演算(デコンボリューション)によって、バイリニア拡大(小さくなった画像サイズをバイリニア補間で元のサイズに拡大)する
                     例：入力サイズが32×10×10(チャンネル数・高さ・横幅)のデータに対する全結合層 → 32×10×10のフィルターサイズの畳み込み層に置き換えられる
                         もしも、出力ノード数が100なら、32×10×10のフィルターを100個用意すれば同じ処理になる


・画像キャプション生成：画像を説明する文章(画像キャプション)を自動で生成する
                      NIC(Neural Image Caption)が代表的なモデルで、CNNとRNN(Recurrent Neural Network, 回帰型)から構成される RNNは過去の情報を記憶し、自然言語や時系列データなどの連続性のあるデータに利用
                      なお、画像と自然言語のような複数の情報を組み合わせて処理することを、「マルチモーダル処理」という


・DCGAN(Deep Convolutional Generative Adversarial Network)：2015年に提案された敵対的生成ネットワークの一種で、教師なし学習
                                                            Generator(生成する人)とDiscriminator(識別する人)の2つのニューラルネットワークを使う


・Deep Q-Network(DQN, 強化学習)：環境の変化によってエージェントが何らかの報酬を得て、より良い報酬が得られるように行動指針を決めていく
                              なお、報酬とは決められたものではなく、「見込みの報酬」で明確な指標から逆算する必要がある
                              Q学習と呼ばれる強化学習のアルゴリズムをベースにし、最適な行動を決定するために、「最適行動価値関数」と呼ばれる関数を使い、それを近似するためにCNNを用いる

・Pickle：Pythonに標準装備されているバイナリフォーマットの保存形式
          あらゆるPythonモジュールをそのままバイナリ化して保存できる
          Pythonやライブラリのバージョンの互換性がない
          Python以外の言語やツールでは使えない

・HDF5(Hierarchical Data Format)：階層的にデータを扱うバイナリフォーマットの保存形式