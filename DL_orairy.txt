 ・(単純)パーセプトロン：隠れ層を持たず、活性化関数はステップ関数
 ・ニューラルネットワーク：隠れ層を持ち、活性化関数はシグモイド関数やReLU関数(Rectified Linear Unit) 多層パーセプトロンともいう

・丸め誤差(rounding error)
 少数の小さな範囲において、数値が省略されることで(例えば小数点第8位以下が省略)、最終的な計算結果に誤差が生じること 10^(-4)なら微小値として問題ない
 np.float32(1e-50) → 0.0

 ・勾配(gradient)
  すべての変数の偏微分をベクトルとしてまとめたものを勾配と呼び、勾配が示す方向は各場所において、関数の値を最も減らす方向である。

・勾配法
 繰り返し勾配方向に進み、関数の値を徐々に減らす方法
    ・勾配降下法：最小値を探す場合
    ・勾配上昇法：最大値を探す場合 ※損失関数の符号を反転させれば、同じなので勾配降下法が一般的

・ハイパーパラメータ：人の手で学習前に決定しておくパラメータ(評価指標,エポック数,学習率,閾値,身にバッチサイズ,層の数など)
・パラメータ：機械学習モデルが学習過程で自動的に調整するもの(重み,バイアスなど)


・連鎖率の原理：合成関数の微分は、それぞれの関数の微分の積(偏微分)によって表せる
                σz/σx = σz/σt ・ σt/σx

・逆伝播法の計算：ノードの入力信号に対して、ノードの局所的な微分(偏微分)を乗算して次のノードに伝達する
                連鎖率の原理を使って目的とする微分の値を効率よく求めることができる
    
    ・加算ノードの逆伝播：z=x+y のとき
                         σz/σx = 1
                         σz/σy = 1 となり、入力された値に1を掛けて、そのまま次のノードに流す
    
    ・乗算ノードの逆伝播： z=xy のとき
                         σz/σx = y
                         σz/σy = x となり、入力された値に順伝播の入力信号をひっくり返した値を掛けて、次のノードに流す
                         10         6.5
                            ↘           ↖
                              50          1.3
                            ↗           ↙
                         5           13

・Affine(全結合層)：入力されたすべての行列の内積(ドット積)と重みの和