畳み込み
・平滑化フィルタ
    画像の各座標における画素値をその周辺の画素値を考慮して更新する方法で、
    ある画素値はその近傍の画素値と近い値を持っている可能性か高い(局所性)を利用して、ノイズ除去に使う
    平均値フィルタ、メディアンフィルタ、ガウシアンフィルタがある
    平滑化フィルタをかける際は、画素値を更新するためどのように周辺の値を収集するかを決める、カーネルを用意 
    カーネルが中心を持つように3×3, 5×5などの奇数が使われる
    カーネルはガウス図んプにおり各値が決められた後、総和が1になるように正規化される

・特徴抽出
    エッジ特徴量: 画像内で輝度の変化が大きい部分を表す特徴量で、物体の輪郭抽出に使われる
    x,y軸方向に微分をすると垂直,水平方向のエッジ特徴量が得られる。
    特徴抽出用カーネル: x軸方向(1次微分)  y軸方向  ラプラシアンフィルタ(2次微分)
                                0 0 0    0 0 0     0 1 0
                                0-1-1    0-1 0     1-4 1
                                0 0 0    0 1 0     0 1 0
    2次微分は輝度の変化量が大きい部分を抽出するため、1次微分ではエッジ特徴量となってしまうような輝度が一致に変化する部分を除外できる


    アテンション: 
    もともと自然言語処理のために開発されたもので、画像のどの部分から特徴を抽出するか動的に決定できる
    畳み込み演算を使った特徴抽出では事前にカーネルを決定する必要があり、入力画像が多様な場合、多数のカーネルを用意する必要がある
    アテンションでは、「クエリ」という特徴収集用のベクトルが入力画像の色に合わせてクエリを作成し、効率的に抽出できる
    特徴量とクエリの内積をとり、ソフトマックスを計算して、アテンションが大きいほど特徴量とクエリの関連度が高く、重要な特徴である

・データセット
 アノテーション: 用意された画像にラベルを付与する作業
 ホールドアウト法: データセットを学習セット(training), 検証セット(validation), テストセット(test)で8:1:1か7:1:2に分割して性能評価を行う方法
                   検証セットを使った学習直後の評価は、ハイパーパラメータチューニングで学習回数やモデルの大きさなどのモデルに最適な設定値を見つける
                   その後、テストセットを用いてモデルの最終性能評価をする

・画像データの整形:
    Numpy配列に変換後、線形関数に入力するために平坦化する。その後、データの各次元が学習セットのデータ全体で平均0, 標準偏差1となるように正規化して整える(標準化)
・ラベルの整形:
    数字表現(0~(x-1)個)と、One-hot表現(0~x個のベクトルを用意して、数字に対応する要素を1, それ以外を0にして学習に必要な計算を簡単にする)
    0       [1,0,0,…,0]
    1       [0,1,0,…,0]
    2       [0,0,1,…,0]
    3       [0,0,0,…,0]
    :           :
    x-1     [0,0,0,…,1]


・モデル  ―パラメータの学習 
    勾配降下法(gradient descent method)：
    学習モデルで予測を行った後、予測に対する目的関数(正解値と予測値の誤差を計算する損失関数など)の値を計算する
    得られた値を使って目的関数のパラメータ方向の勾配を計算し、値を減少させる方向にパラメータを更新して最適解(谷の底)を求める
    ・局所的最適解: 近傍で目的関数が最小となる解
    ・大域的最適解: 近傍だけでなく、すべての解の中で目的関数が最小となる解
    勾配降下法では大域的最適解にまで谷を下ることが目標
    

    交差エントロピー誤差(cross entropy error):
        多クラスロジスティック回帰の目的関数に使われる
        真の確率分布(One-hot表現のラベル)と予測された確率分布(ソフトマックスの出力)が一致するときに最小となる
        真の確率分布では、正しい物体クラスの確率が1, それ以外の確率が0であるため、ソフトマックスの出力が正しい物体クラスに1,それ以外に0の値を持つよう学習される


    
    勾配降下法から派生した最適化アルゴリズム(Optimizer): サンプル数によって3種類に分類される
        1. バッチ勾配降下法(batch): 全ての学習セットのサンプル
        2. 確率的勾配降下法(stochastic, SGD): 無作為に1つのサンプル
        3. ミニバッチ確率的勾配降下法(mini-batch): 無作為にB個のサンプル(Bはバッチサイズと呼ばれるハイパーパラメータ、慣習的に32,64,128,256,512...が使われる)
            1の欠点:
                1回のパラメータ更新ですべてのサンプルを使って目的関数と勾配を計算するため、計算量が非常に大きく、
                学習結果がパラメータの初期値に依存する(最初の谷(局所的最適解)から抜け出せない)
            2の欠点:
                1回のパラメータ更新あたりの計算量は小さく、次のサンプルで勾配が0でなくなる可能性がある(局所的最適解から抜け出せるかも)
                しかし、勾配の向きのばらつきが大きく、最適解までたどり着くのに時間がかかる
            3:
                バッチサイズBを変えることで、1回のパラメータ更新あたりの計算量を調整できる
                複数サンプルで勾配を計算して勾配の方向を安定させつつ、無作為な抽出により局所的最適解からの離脱も可能

    勾配の計算:
        連鎖率(chain rule)やベクトル同士の偏微分を使う

・学習・評価
    ・エポック数(epoch:学習回数)：
        学習セットの全サンプルを抽出した回数
    ・イテレーション数(iteration:反復)：
        ミニバッチを抽出した回数 1イテレーションで1回勾配が計算され、パラメータが更新される
    例→データセットが1000, バッチサイズ200ならイテレーションは5(5回学習を繰り返す) エポック数はこの手順を行う回数

    ・早期停止：
        学習が進むと検証セットの目的関数は減少していくが、ある時点を境に上昇し過学習となる(学習セットのデータのノイズにも過剰に適合するため)
        そのため、目的関数の値が上昇し始める直前のエポック数を最適とする



深層学習: 深層ニューラルネットワーク(Deep Neural Network, DNN)を使った機械学習のこと
 活性化関数φ:
    非線形
    ・シグモイド関数(sigmoid):出力は0~1 出力層の後でよく使われる
    ・双曲線正接関数(tanh):出力は-1~1
    ・正規化線形ユニット関数(Rectified Linear Unit, ReLu):出力は x≦0で1,0<xでx  一般的な層でよく使われる
    線形
    ・ソフトマックス関数: 出力層の後でよく使われる

隠れ層→特徴抽出
出力層→分類・回帰 とみなせる

誤差逆伝播法(error backpropagation method):
    目的関数の勾配を出力層から計算していき、入力層に伝播するパラメータの更新方法
    出力を入力で微分し、後ろから伝播されてきた勾配との間で積を計算する
    逆伝播中の偏微分の計算には3つの活性化関数が含まれ、一般的な層ではRELU関数を用いる←勾配消失問題が関係する
    ・シグモイド関数の偏微分: 0~0.25の正規分布
    ・tanh関数の偏微分: 0~1.0の正規分布
    ・RELU関数の偏微分: 0か1のみをとる不連続な関数

    勾配消失問題：
        誤差逆伝播法では出力層から入力層にかけて勾配(微分)値の積を計算した
        1よりも小さい微分値を何回もかけていくと、勾配の絶対値は指数関数的に減少し、入力に近い層の勾配は0に近づく(勾配消失)
        勾配消失が起こると、パラメータがほとんど更新されず、学習が進まない
        →RELU関数を使えば、入力が0より大きいときの微分値は1であるため、出力に近い層の勾配が入力層のほうへ減衰せずに伝播される
         通常1つのノード(ニューロン)は次の層の複数のノードに接続されており、それらすべてのノードのRELU関数の微分値が0,0以下になる可能性は低く、勾配消失が起きにくい

        一般的な層ではRELU関数だが、出力層の後の活性化関数はシグモイド関数,ソフトマックス関数がよく使われる(出力を目的のタスクに合わせて調整するため)
        シグモイド関数やソフトマックス関数を使うと勾配の絶対値が減衰しやすいが、
        目的関数に二乗誤差ではなく、交差エントロピー誤差を使うことで伝播される勾配の絶対値が大きくなり、勾配消失の問題を軽減できる


DNNの種類
・FNN(順伝播型ニューラルネットワーク, Feedforward Neural Network):
    最も基本的なニューラルネットワークであり、多層パーセプトロン(MultiLayer Perceptrons, MLP)とも呼ばれる
    前後の層のノード同士が全て接続されている全結合層をベースとすることが多い

・CNN(畳み込みニューラルネットワーク, Convolutional Neural Network):
    畳み込み演算を活用したニューラルネットワークであり、画像認識でよく使われる

・RNN(再帰型ニューラルネットワーク, Recurrent Neural Network):
    再帰的に入力ベクトルの合成を繰り返すニューラルネットワークであり、画像キャプショニングのような時系列予測でよく使われる

・Transformer:
    アテンション機構(画像のどの部分から特徴を抽出するか動的に決定できる)を活用したネットワークであり、自然言語処理向けに開発されたが、画像認識でも活用される