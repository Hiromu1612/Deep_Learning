テンソル： 数値データが格納されるコンテナのようなもので、Numpy配列でよく表される
           次元(dimension)は軸(axis)と呼ばれる
    ・スカラー(0次元テンソル):Numpyの数値を1つしか格納していないテンソル ndim=0 np.array(10)
    ・ベクトル(1次元テンソル):Numpyの1次元配列 np.array([1,2,3])
    ・行列(2次元テンソル):    Numpyの2次元配列(行列) np.array([1,2,3],
                                                        [4,5,6])

    データテンソルの例
    ・ベクトルデータ              ：形状が(samples, features)の2次元テンソル
    ・時系列,シーケンス(系列)データ：形状が(samples, timesteps, features)の3次元テンソル
    ・画像                        ：形状が(samples, height, width, channels)の4次元テンソル
    ・動画                        ：形状が(samples, frames, height, width, channels)の5次元テンソル

        ベクトルデータは、最も一般的なデータで、サンプル軸と特徴軸の2次元 (例：人の年齢,住所,収入などの表データ)
        時系列データは、時間が重要となるので3軸目に時間軸を追加し、データはサンプル軸と特徴軸 (例：株価のデータ)
        画像データは、幅・高さ・色深度の3つの次元で表され、4次元目はカラーチャネルで1か3


一般的流れ
1. モデルの構築: ニューラルネットワークの構成、活性化関数、損失関数などを設定
2. 順伝播: 入力データから出力データまでの計算
3. 損失関数の計算: 交差エントロピー誤差などを用いて、予測と真の分布の差異を計算
4. 勾配降下法によるパラメータ更新: 損失関数を最小化する方向にパラメータを更新 ミニバッチ確率的勾配降下法などのオプティマイザ
5. 2～4を繰り返す: 損失関数が収束するまで学習を継続


・損失関数(loss 目的関数の種類)：訓練中の予測結果と正解との誤差を計算する数式
    回帰
    ・平均二乗誤差(Mean Square Error, MSE):回帰でよく使われる
    ・平均絶対誤差(Mean Absolute Error, MAE): 0に近いほど良い
    ・平均二乗対数誤差(Mean Squared Logarithmic Error, MSLE)
    
    分類
    ・交差エントロピー誤差(cross entropy error)：エントロピーとは「分布の類似度を表す指標」で正解に近いほど0に近づく
    ・2値交差エントロピー誤差(binary cross entropy error):2クラス分類でよく使われる
    ・多クラス交差エントロピー誤差(categorical cross entropy):多クラス分類でよく使われる　ラベルがone-hot表現のとき
    ・スパース多クラス交差エントロピー誤差(sparce categorical cross entoropy error):多クラス分類でよく使われる　ラベルが整数のとき


・最適化アルゴリズム(optimizer):
    1. バッチ勾配降下法(batch): 全ての学習セットのサンプル
    2. 確率的勾配降下法(Stochastic Gradient Descent, SGD): 無作為に1つのサンプル
    3. ミニバッチ確率的勾配降下法(mini-batch): 無作為にB個のサンプル(Bはバッチサイズと呼ばれるハイパーパラメータ、慣習的に32,64,128,256,512...が使われる)
    4. モーメンタム(momentum): 勾配の移動平均をとり、SGDの振動(momentum)を抑える
    5. RMSProp: モーメンタムと同じ目的で、勾配の大きさに応じて振動方向の学習率を調整してSGDの振動を抑える
    6. Adam(Adaptive moment estimation): 4のモーメンタムと5のRMSPropの良いとこどり。 移動平均と振動方向の学習率調整でSGDの振動を抑制


・評価関数(metrics):
    ・正解率(accuracy, acc): 分類でよく使われる
    ・平均絶対誤差(Mean Absolute Error, mae): 回帰でよく使われる そもそも回帰には正解率の概念がない


model.compile(optimizer = "rmsprop",
              loss = "binary_crossentropy",
              metrics = ["accuracy"])
    オプティマイザ,損失関数,指標が文字列で渡せるのは、Kerasの一部としてパッケージ化されているため

history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))
    historyの辞書には訓練中に起きたすべてに関するデータが含まれる。Historyオブジェクトのhistoryというメンバーにある。
    辞書の中身は["val_acc", "acc", "val_loss", "loss"]
    val_accは検証中の精度(validation)
    accは訓練中の精度
    val_lossは検証中の損失
    lossは訓練中の損失


・ハイパーパラメータ：モデルのパラメータ　人が学習前に決める
・パラメータ：ネットワークの重み 機械学習モデルが学習中に決める重み


・学習モデルの評価(ハイパーパラメータのチューニング)
    ・ホールドアウト法:
        データセットを学習セット(training), 検証セット(validation), テストセット(test)で8:1:1か7:1:2に分割して性能を評価

    ・k分割交差検証(k-Fold cross-validation):
        データセットをk個のサブセット(Fold)に分割し、k-1個のサブセットを訓練データ、1個のサブセットを検証データにするのをk回繰り返し、正解率や誤差の平均値で評価
    
    ・グリッドサーチ：
        すべてのパラメータの組み合わせから最も精度の高いものを探す

    ・ランダムサーチ：
        指定した最大回数の範囲でランダムにパラメータを探す
    
    ・ベイズ最適化：
        前回のパラメータの組み合わせを参考に効率よく探す


・ニューラルネットワークにおけるデータの前処理
    ・ベクトル化：
        ニューラルネットワークの入力値と目的値(真値)は、浮動小数点データ(整数の場合も稀にある)のテンソル(数値データのコンテナ)でなければならない
        音声,画像,テキストなどすべてはテンソルに変換する必要がある(データのベクトル化)
        例; テキストが整数のリストの場合→one-hotエンコーディングを使ってfloat32型のテンソルに変換

    ・値の正規化・標準化：
        正規化 = 平均値0, 分散1　　  機械学習の回帰モデル
        標準化 = 最小値0, 最大値1　　画像処理(0~255)
        例; 手書き数字の0~255のグレースケールの整数値→255で割ってfloat32型のテンソルに標準化
            m=x.mean()
            s=x.std()
            x=(x-m)/s

    ・欠測値：
        一般に、ニューラルネットワークでは欠測値は0にするのが安全。そのネットワークは「0が欠測値」ということを学習して無視するようになる
        訓練データに欠測値が含まれず、テストデータに含まれる場合、学習できていないためテストデータで欠測値になりそうな特徴量の一部を0にする


・特徴量エンジニアリング：
    使用しているデータと機械学習アルゴリズム(今回はニューラルネットワーク)に関する知識に基づいて、そのアルゴリズムの性能を向上させるプロセス
    特徴量をより単純な方法で表現することで機械学習を単純化する
        例→時計の時刻を読み取る場合、複雑なCNNではなく、時計の針の座標を極座標変換した方が簡単
    従来のシャロ―ラーニング(表層学習)では特徴量エンジニアリングが重要だったが、ディープラーニングは有益な特徴量を自動抽出するため、必要なくなった


・過学習と学習不足
    ・最適化：訓練データでの性能を高めるためにモデルを調整すること
    ・汎化  ：学習済みモデルを新しいデータに適用したときの性能のよさ
    