脳は「ニューロン」という神経細胞でできていて、1つの大きさは0.005~0.1mmで、脳全体で1000億個もあるといわれる
樹状突起・軸索(長いケーブル)・軸索末端(端子)があり、樹状突起から「入力」された信号が、軸索末端から「出力」される
ニューロンは受け取った信号が閾値以上のとき、活動が活性化して次のニューロンに電気信号を出力する
シナプス(ニューロンのつなぎ目)は頻繁に信号が流れることでつながりが強化される→「パーセプトロン(古典的な人工ニューロン)」
パーセプトロン一つでは2分割しかできないため、人工ニューロンをつなぎ合わせた「人工ニューラルネットワーク(ANN)」で、入力層・中間層(隠れ層)・出力層
1986年に誤差逆伝播法が提唱される
その後、「勾配消失問題」という「層を多くすると学習が上手くいかなくなる」問題が発生したが、
2006年に「オートエンコーダー」で解決し、層が2-3層以上あるものを「ディープラーニング」と呼ぶようになった

誤差逆伝播法
・最初のニューロンの重みはランダムで、誤差の大小によって修正する
・損失関数で、誤差の大きさが分かる
・最適化アルゴリズムの一つ、勾配法で傾きが右上がりなら重みを小さい方へ移動させ小さくしていく
・逆伝播によって重みを修正する
・問題に答えて誤差で重みを修正する手順を1回の学習と考えて、エポック数(Epoch)と呼ぶ

活性化関数
・入力の合計値が閾値を超えたら「活動が活性化して」出力する関数
    パーセプトロンは単純で「ステップ関数」と呼び、直線的で0か１、傾きは水平で間違いの修正が上手くできない
・例：シグモイド関数;0~1で滑らかな曲線
      ハイパボリックタンジェント関数;-1~1でより複雑な表現ができ、学習スピードも上がる
      ReLU関数;0より大きいときはそのまま、小さいときは0 計算がシンプルでさらに高速に計算できる


CNN(Convolutional Neural Network)
・1979年に福島博士が目の細胞をヒントに「ネオコグニトロン」を考案して、それを改良したのがCNN
 目の細胞には、「S細胞(単純型細胞)」と「C細胞(複雑型細胞)」がある
 S細胞: 画像の濃淡から画像の特徴を抽出できる    → 畳み込み層(Convolution)
 C細胞: 画像の位置が多少ずれていても吸収できる  → プーリング層(Pooling)

 畳み込み層: 画像に小さなフィルタ(カーネル)をスライドさせて、パターンを強調させる 「フィルタ処理」という画像処理を行う
 プーリング層: 2×2の範囲の最大値を取り出して、画像を縮小させる 「ダウンサンプリング」という画像処理を行う
 model.add(layers.Conv2D(32,64(フィルタ数), (5,5), activation=活性化関数))   32×32×3はRGB画像
 model.add(layers.MaxPooling2D((2,2)))

 ドロップアウト層: 訓練データに過剰に適合する(過学習)のを防ぐために、ニューロンをランダムに削除し、ネットワーク全体の複雑さを減らす
 model.add(layers.Dropout(0.2)) 20%ドロップアウトさせて過学習を予防